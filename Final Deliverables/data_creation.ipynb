{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Dufvw9W8l-E"
      },
      "outputs": [],
      "source": [
        "dataset = pd.read_csv(\"data.csv\")\n",
        "inp = dataset[\"input_text\"]\n",
        "tar = dataset[\"target_text\"]\n",
        "split = dataset[\"split\"]\n",
        "trn_df = pd.DataFrame(columns=[\"input_text\",\"target_text\"])\n",
        "test_df = pd.DataFrame(columns=[\"input_text\",\"target_text\"])\n",
        "val_df = pd.DataFrame(columns=[\"input_text\",\"target_text\"])\n",
        "for i in range(len(split)):\n",
        "  if split[i]==\"train\":\n",
        "    trn_df = trn_df.append({\"input_text\":inp[i],\"target_text\":tar[i]},ignore_index=True)\n",
        "  elif split[i]==\"val\":\n",
        "    val_df = val_df.append({\"input_text\":inp[i],\"target_text\":tar[i]},ignore_index=True)\n",
        "  else:\n",
        "    test_df = test_df.append({\"input_text\":inp[i],\"target_text\":tar[i]},ignore_index=True)\n",
        "trn_df.to_csv(\"train.csv\")\n",
        "test_df.to_csv(\"test.csv\")\n",
        "val_df.to_csv(\"val.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import os\n",
        "import torch\n",
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "def con(in_d,out_d):\n",
        "  csv_file = in_d  # Path to your CSV file\n",
        "  output_file = out_d # Path to save formatted data\n",
        "  tokenizer = GPT2Tokenizer.from_pretrained(\"gpt2\")  # Tokenizer for GPT-2\n",
        "\n",
        "  # Read CSV file\n",
        "  with open(csv_file, \"r\", encoding=\"utf-8\") as f:\n",
        "      reader = csv.DictReader(f)\n",
        "      rows = [row for row in reader]\n",
        "\n",
        "  # Format data for GPT-2\n",
        "  formatted_rows = []\n",
        "  for row in rows:\n",
        "      input_text = row[\"input_text\"]\n",
        "      target_text = row[\"target_text\"]\n",
        "      formatted_text = f\"question: {input_text.strip()} context: {target_text.strip()} {tokenizer.eos_token}\\n\"\n",
        "      formatted_rows.append(formatted_text)\n",
        "\n",
        "  # Save formatted data\n",
        "  with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
        "      f.writelines(formatted_rows)"
      ],
      "metadata": {
        "id": "Xk2NyAuL9_OQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "con(\"train.csv\",\"train.txt\")\n",
        "con(\"test.csv\",\"test.txt\")"
      ],
      "metadata": {
        "id": "35luwDSy-CJK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}